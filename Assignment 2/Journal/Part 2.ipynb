{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54789b33-f1d5-4211-91cb-1528ff3b1dc6",
   "metadata": {},
   "source": [
    " # Comprehensive Documentation and Execution Plan for the Sessa Empirical Estimator (SEE) Assignment\n",
    "\n",
    " This notebook serves as a robust, highly detailed documentation of our learning, planning, and methodology for the SEE assignment. It is designed for both industry professionals and educational purposes.\n",
    "\n",
    " In this notebook, we document:\n",
    "\n",
    " 1. **Assignment Overview and Objectives:**\n",
    "    - Understand the SEE method and its applications.\n",
    "    - Convert the SEE R code to Python.\n",
    "    - Apply the method on simulated or real-world data.\n",
    "    - Explore alternative clustering methods and compare outcomes.\n",
    "\n",
    " 2. **Literature and Source Material Review:**\n",
    "    - Key insights extracted from three critical PDFs.\n",
    "\n",
    " 3. **Detailed Methodology and Execution Plan:**\n",
    "    - Step-by-step breakdown covering data preprocessing, ECDF computation, clustering (k‑means and alternatives), and duration assignment.\n",
    "    - Detailed explanations and mathematical rationales.\n",
    "\n",
    " 4. **Code Translation and Modular Function Design:**\n",
    "    - Modular functions with thorough inline comments.\n",
    "    - Enhanced to support both sample datasets.\n",
    "\n",
    " 5. **Validation, Visualization, and Reporting:**\n",
    "    - Robust validation and visualization strategies.\n",
    "\n",
    " 6. **Handling the Sample Datasets:**\n",
    "    - **med.events:**\n",
    "      - Contains 1080 rows for 100 patients with variables: PATIENT_ID, DATE, PERDAY, CATEGORY, DURATION.\n",
    "      - Represents individual medication events used for adherence analyses.\n",
    "    - **med.events.ATC:**\n",
    "      - Contains 1564 rows for 16 patients and extends med.events with ATC codes and hierarchical categorizations (CATEGORY_L1 and CATEGORY_L2).\n",
    "\n",
    " 7. **Conclusion and Next Steps:**\n",
    "    - Summary and future directions.\n",
    "\n",
    " **Table of Contents:**\n",
    "\n",
    " 1. [Assignment Overview and Objectives](#overview)\n",
    " 2. [Literature and Source Material Review](#literature)\n",
    " 3. [Detailed Methodology and Execution Plan](#methodology)\n",
    " 4. [Code Translation and Modular Function Design](#code_design)\n",
    " 5. [Validation, Visualization, and Reporting](#validation)\n",
    " 6. [Handling the Sample Datasets](#sample_datasets)\n",
    " 7. [Conclusion and Next Steps](#conclusion)\n",
    "\n",
    " ---\n",
    "\n",
    " <a id=\"overview\"></a>\n",
    " ## 1. Assignment Overview and Objectives\n",
    "\n",
    " **Objective:**\n",
    " Compute the duration of pharmacological prescriptions using the data‑driven Sessa Empirical Estimator (SEE), particularly when key data such as prescribed dose or daily consumption is missing.\n",
    "\n",
    " **Key Tasks:**\n",
    "\n",
    " - **Literature Review:**\n",
    "   Understand the SEE methodology from key PDFs.\n",
    "\n",
    " - **R to Python Conversion:**\n",
    "   Translate the SEE R code to Python within a Jupyter Notebook.\n",
    "\n",
    " - **Data Application:**\n",
    "   Apply the method on simulated or real-world datasets (including the provided med.events and med.events.ATC).\n",
    "\n",
    " - **Alternative Clustering Exploration:**\n",
    "   Evaluate alternative clustering algorithms (e.g., DBSCAN, hierarchical clustering, Gaussian Mixture Models) and compare them to k‑means.\n",
    "\n",
    " - **Modular & Reproducible Code:**\n",
    "   Develop callable functions for every major step to ensure transparency, reproducibility, and ease of collaboration.\n",
    "\n",
    " - **Comprehensive Reporting:**\n",
    "   Document every step and decision for robust educational and industry-level documentation.\n",
    "\n",
    " ---\n",
    "\n",
    " <a id=\"literature\"></a>\n",
    " ## 2. Literature and Source Material Review\n",
    "\n",
    " We have reviewed three PDFs that provide the following insights:\n",
    "\n",
    " 1. **PHARMACOM-EPI Framework (First PDF):**\n",
    "    - Validates safety signals (e.g., lamotrigine toxicity) via plasma concentration predictions.\n",
    "    - Emphasizes rigorous data preprocessing and model validation.\n",
    "\n",
    " 2. **Co-exposure Assessment (Second PDF):**\n",
    "    - Describes a method for assessing co‑exposure to free dose antihypertensive medications.\n",
    "    - Highlights construction of treatment episodes, overlapping period computations, and timeline visualizations.\n",
    "\n",
    " 3. **SEE Methodology (Third PDF):**\n",
    "    - Introduces the SEE method for computing prescription durations using ECDF and k‑means clustering.\n",
    "    - Provides detailed rationale for ECDF trimming (removing upper 20%) and optimal cluster selection via silhouette analysis.\n",
    "\n",
    " **Key Takeaways:**\n",
    "\n",
    " - Use a data‑driven approach to minimize assumptions.\n",
    " - Rigorous preprocessing is essential: ordering by patient, converting dates, and computing refill intervals.\n",
    " - The clustering process and subsequent duration assignment are central to accurate exposure estimation.\n",
    " - Flexibility to explore alternative clustering methods is valuable.\n",
    "\n",
    " ---\n",
    "\n",
    " <a id=\"methodology\"></a>\n",
    " ## 3. Detailed Methodology and Execution Plan\n",
    "\n",
    " **Step 1: Deep Dive into SEE Methodology and Literature**\n",
    " - Re-read the PDFs to grasp every step: ECDF computation, trimming, clustering, and duration assignment.\n",
    " - Understand mathematical details:\n",
    "    - **ECDF Trimming:** Excludes the top 20% of intervals to avoid skew from outliers.\n",
    "    - **Silhouette Analysis:** Objectively determines the optimal number of clusters.\n",
    "\n",
    " **Step 2: Environment Setup and R to Python Translation**\n",
    " - Set up Jupyter Notebook with required libraries:\n",
    "   - Data handling: `pandas`, `numpy`\n",
    "   - Clustering: `scikit-learn`\n",
    "   - Visualization: `matplotlib`, `seaborn`\n",
    " - Translate the SEE R code into Python.\n",
    "\n",
    " **Step 3: Data Acquisition and Application**\n",
    " - Select or simulate a dataset (e.g., using med.events or med.events.ATC).\n",
    " - Apply the modular functions to compute prescription durations.\n",
    "\n",
    " **Step 4: Alternative Clustering Exploration**\n",
    " - Implement alternative clustering methods (DBSCAN, hierarchical clustering, GMM).\n",
    " - Compare performance via silhouette scores and visualizations.\n",
    "\n",
    " **Step 5: Comparison and Reporting**\n",
    " - Compare outputs of k‑means versus alternative methods.\n",
    " - Generate detailed visualizations (ECDF, density plots, timeline diagrams) to support findings.\n",
    "\n",
    " **Step 6: Modular Callable Functions and Final Checks**\n",
    " - Ensure each process is encapsulated in well-documented, callable functions.\n",
    " - Validate consistency across interactive and programmatic runs.\n",
    "\n",
    " **Step 7: Final Submission Preparation**\n",
    " - Compile the final notebook, report, and supporting files.\n",
    " - Ensure thorough documentation and meet the assignment deadline.\n",
    "\n",
    " ---\n",
    "\n",
    " <a id=\"code_design\"></a>\n",
    " ## 4. Code Translation and Modular Function Design\n",
    "\n",
    " The following sections provide modular functions with extensive comments. They are designed to handle both sample datasets:\n",
    " - **med.events:** Contains PATIENT_ID, DATE, PERDAY, CATEGORY, DURATION.\n",
    " - **med.events.ATC:** Contains additional fields: CATEGORY_L1 and CATEGORY_L2.\n",
    "\n",
    " This flexibility ensures our pipeline can process data in either format.\n",
    "\n",
    " ### 4.1. Data Preprocessing Function\n",
    "\n",
    " This function now includes parameters to specify dataset types and handles column name differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf7b5e-ce23-4ef6-bfba-ac047cc8f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def preprocess_data(df, dataset_type='med.events', patient_col=None, date_col=None):\n",
    "    \"\"\"\n",
    "    Preprocesses prescription data by converting date columns, sorting, and computing intervals between events.\n",
    "    \n",
    "    Supports two dataset formats:\n",
    "    - 'med.events': Expects columns: PATIENT_ID, DATE, PERDAY, CATEGORY, DURATION.\n",
    "    - 'med.events.ATC': Expects the above plus CATEGORY_L1 and CATEGORY_L2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame containing prescription data.\n",
    "    dataset_type : str\n",
    "        Type of dataset: 'med.events' or 'med.events.ATC'. Default is 'med.events'.\n",
    "    patient_col : str or None\n",
    "        Column name for patient identifier. If None, defaults are used:\n",
    "          - 'PATIENT_ID' for med.events.\n",
    "    date_col : str or None\n",
    "        Column name for the prescription date. If None, defaults are used:\n",
    "          - 'DATE' for med.events.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with additional columns:\n",
    "        - 'prev_date': Previous prescription date for each patient.\n",
    "        - 'event_interval': Interval in days between consecutive prescriptions.\n",
    "    \"\"\"\n",
    "    # Set default column names if not provided\n",
    "    if patient_col is None:\n",
    "        patient_col = 'PATIENT_ID'\n",
    "    if date_col is None:\n",
    "        date_col = 'DATE'\n",
    "        \n",
    "    # Convert the date column to datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col], format='%m/%d/%Y')\n",
    "    \n",
    "    # Sort the DataFrame by patient identifier and prescription date\n",
    "    df.sort_values(by=[patient_col, date_col], inplace=True)\n",
    "    \n",
    "    # Compute previous prescription date for each patient\n",
    "    df['prev_date'] = df.groupby(patient_col)[date_col].shift(1)\n",
    "    \n",
    "    # Calculate the event interval in days between consecutive prescriptions\n",
    "    df['event_interval'] = (df[date_col] - df['prev_date']).dt.days\n",
    "    \n",
    "    # Drop rows where event_interval is missing (first prescription for each patient)\n",
    "    df = df.dropna(subset=['event_interval'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679bce4-9b60-4bbe-a6a0-3126420445cf",
   "metadata": {},
   "source": [
    " ### 4.2. ECDF Computation and Trimming Function\n",
    "\n",
    " This function computes the ECDF for the refill intervals and trims the upper 20%.\n",
    "\n",
    " **Mathematical Details:**\n",
    " - ECDF is computed by ranking sorted intervals and dividing by the total count.\n",
    " - Trimming removes the highest 20% of values to avoid skewing the median calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f5d0f-40ea-4cc9-a555-81b9b651cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def compute_trimmed_ecdf(intervals, trim_fraction=0.8):\n",
    "    \"\"\"\n",
    "    Computes the ECDF for an array of intervals and trims it to retain the lower fraction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of time intervals between prescriptions.\n",
    "    trim_fraction : float\n",
    "        Fraction of the ECDF to retain (default 0.8 retains lower 80%).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trimmed_intervals : numpy.array\n",
    "        Array of intervals within the trimmed ECDF.\n",
    "    \"\"\"\n",
    "    sorted_intervals = np.sort(intervals)\n",
    "    ecdf = np.arange(1, len(sorted_intervals)+1) / len(sorted_intervals)\n",
    "    trimmed_intervals = sorted_intervals[ecdf <= trim_fraction]\n",
    "    \n",
    "    return trimmed_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063909a8-c3ea-49ea-95b1-532618283902",
   "metadata": {},
   "source": [
    " ### 4.3. Standardization and K-Means Clustering Function\n",
    "\n",
    " This function standardizes the intervals, applies k‑means clustering, and uses silhouette analysis to determine the optimal number of clusters.\n",
    "\n",
    " **Key Concepts:**\n",
    " - **Standardization:** Normalizes data (subtract mean, divide by standard deviation).\n",
    " - **Silhouette Score:** Measures cluster quality; higher values indicate better clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e2f30-f7bf-427c-b208-ccf768929918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def perform_kmeans_clustering(intervals, max_clusters=10, random_state=123):\n",
    "    \"\"\"\n",
    "    Standardizes intervals and applies k-means clustering, selecting the optimal number of clusters using silhouette analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of time intervals.\n",
    "    max_clusters : int\n",
    "        Maximum clusters to test.\n",
    "    random_state : int\n",
    "        Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_k : int\n",
    "        Optimal number of clusters.\n",
    "    cluster_labels : numpy.array\n",
    "        Labels assigned to each interval.\n",
    "    kmeans_model : KMeans\n",
    "        Trained k-means model.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    intervals_std = scaler.fit_transform(np.array(intervals).reshape(-1, 1))\n",
    "    \n",
    "    best_score = -1\n",
    "    optimal_k = 2\n",
    "    best_labels = None\n",
    "    best_model = None\n",
    "    \n",
    "    for k in range(2, max_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "        labels = kmeans.fit_predict(intervals_std)\n",
    "        score = silhouette_score(intervals_std, labels)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            optimal_k = k\n",
    "            best_labels = labels\n",
    "            best_model = kmeans\n",
    "    \n",
    "    return optimal_k, best_labels, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f165c-85d7-4c12-82c2-bc474aa4a097",
   "metadata": {},
   "source": [
    " ### 4.4. Compute Cluster Medians and Assign Prescription Durations\n",
    "\n",
    " This function calculates the median interval for each cluster, which serves as the computed prescription duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d79a14-a5f4-400a-a0da-e0371a764c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def assign_durations(intervals, labels):\n",
    "    \"\"\"\n",
    "    Computes the median interval for each cluster and returns a mapping from cluster label to median duration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of time intervals.\n",
    "    labels : array-like\n",
    "        Cluster labels for each interval.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_medians : dict\n",
    "        Dictionary mapping cluster labels to median intervals.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'interval': intervals, 'cluster': labels})\n",
    "    cluster_medians = df.groupby('cluster')['interval'].median().to_dict()\n",
    "    return cluster_medians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc12117-a5f1-452e-ac32-a32d030f974f",
   "metadata": {},
   "source": [
    " ### 4.5. Alternative Clustering Approaches (Overview)\n",
    "\n",
    " In addition to k‑means, alternative clustering methods can be explored:\n",
    "\n",
    " **DBSCAN:**\n",
    " - Does not require specifying the number of clusters.\n",
    " - Identifies clusters based on density, detecting outliers.\n",
    "\n",
    " **Hierarchical Clustering:**\n",
    " - Builds a dendrogram representing nested clusters.\n",
    " - Can be visually inspected to decide the optimal number of clusters.\n",
    "\n",
    " **Gaussian Mixture Models (GMM):**\n",
    " - Provides a probabilistic framework allowing overlapping clusters.\n",
    "\n",
    " These methods follow a similar structure: standardize, cluster, evaluate, and visualize.\n",
    "\n",
    " For this assignment, k‑means is our primary method; however, our modular design allows easy integration of alternative methods.\n",
    "\n",
    " ---\n",
    "\n",
    " <a id=\"validation\"></a>\n",
    " ## 5. Validation, Visualization, and Reporting\n",
    "\n",
    " **Validation Strategies:**\n",
    "\n",
    " - **ECDF Visualization:**\n",
    "   Plot full and trimmed ECDFs to verify the upper 20% removal.\n",
    "\n",
    " - **Clustering Visualizations:**\n",
    "   Visualize clusters with scatter or density plots; use dendrograms for hierarchical clustering.\n",
    "\n",
    " - **Performance Metrics:**\n",
    "   Compare silhouette scores for different clustering approaches.\n",
    "\n",
    " - **Edge Case Handling:**\n",
    "   Ensure functions work well with small or highly variable datasets.\n",
    "\n",
    " ### Example: ECDF Plotting Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e7897-5c0a-47fd-859e-bdffea7c7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ecdf(intervals, trimmed_intervals, trim_fraction=0.8):\n",
    "    \"\"\"\n",
    "    Plots the full ECDF of intervals and marks the trimming threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Full array of intervals.\n",
    "    trimmed_intervals : array-like\n",
    "        Array after trimming.\n",
    "    trim_fraction : float\n",
    "        Fraction of ECDF retained.\n",
    "    \"\"\"\n",
    "    sorted_intervals = np.sort(intervals)\n",
    "    ecdf_full = np.arange(1, len(sorted_intervals)+1) / len(sorted_intervals)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.step(sorted_intervals, ecdf_full, label='Full ECDF', where='post')\n",
    "    \n",
    "    max_interval_trimmed = trimmed_intervals.max()\n",
    "    plt.axvline(x=max_interval_trimmed, color='red', linestyle='--', \n",
    "                label=f'Trim Threshold (retain <= {trim_fraction*100:.0f}%)')\n",
    "    plt.xlabel('Time Interval (days)')\n",
    "    plt.ylabel('ECDF')\n",
    "    plt.title('ECDF of Prescription Intervals with Trimming')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ff7a1-d0b7-4894-b0f4-440086972a76",
   "metadata": {},
   "source": [
    " <a id=\"sample_datasets\"></a>\n",
    " ## 6. Handling the Sample Datasets: med.events and med.events.ATC\n",
    "\n",
    " **med.events Dataset:**\n",
    " - Contains 1080 rows and 5 variables:\n",
    "   - **PATIENT_ID:** Unique patient identifier.\n",
    "   - **DATE:** Medication event date (mm/dd/yyyy format).\n",
    "   - **PERDAY:** Daily dosage prescribed.\n",
    "   - **CATEGORY:** Medication type (e.g., 'medA', 'medB').\n",
    "   - **DURATION:** Duration of medication event in days.\n",
    "\n",
    " **med.events.ATC Dataset:**\n",
    " - Contains 1564 rows and 7 variables:\n",
    "   - **PATIENT_ID:** Unique patient identifier.\n",
    "   - **DATE:** Medication event date.\n",
    "   - **DURATION:** Duration in days.\n",
    "   - **PERDAY:** Daily dosage.\n",
    "   - **CATEGORY:** ATC code.\n",
    "   - **CATEGORY_L1:** First-level ATC category (e.g., \"A\" for ALIMENTARY TRACT AND METABOLISM).\n",
    "   - **CATEGORY_L2:** Second-level ATC category (e.g., \"A02\" for DRUGS FOR ACID RELATED DISORDERS).\n",
    "\n",
    " **Enhancements for Dataset Handling:**\n",
    "\n",
    " - Our preprocessing function (preprocess_data) can now accept a parameter `dataset_type` to handle differences.\n",
    " - We will add example code for loading and inspecting these datasets.\n",
    "\n",
    " ### Example: Loading and Inspecting a Sample Dataset\n",
    "\n",
    " Assume the datasets are provided as CSV files. The following code loads and previews the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b02aeb-5bf0-48d0-9974-d18db74777bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Example code to load and preview med.events dataset\n",
    "# Uncomment and modify the file path as needed\n",
    "# med_events_df = pd.read_csv(\"path_to_med.events.csv\")\n",
    "# print(\"med.events dataset preview:\")\n",
    "# print(med_events_df.head())\n",
    "#\n",
    "# # Similarly, load med.events.ATC dataset\n",
    "# med_events_atc_df = pd.read_csv(\"path_to_med.events.ATC.csv\")\n",
    "# print(\"med.events.ATC dataset preview:\")\n",
    "# print(med_events_atc_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ff28d-fc5c-4ee8-9aa3-e487ff351621",
   "metadata": {},
   "source": [
    " ## 7. Conclusion and Next Steps\n",
    "\n",
    " **Summary:**\n",
    "\n",
    " - Our notebook robustly documents the SEE methodology, from literature review to execution.\n",
    " - We have modular functions for data preprocessing, ECDF computation, clustering, and duration assignment.\n",
    " - Enhancements have been made to handle the provided sample datasets (med.events and med.events.ATC) with detailed documentation.\n",
    "\n",
    " **Next Steps:**\n",
    "\n",
    " 1. **Integration and Testing:**\n",
    "    Combine the functions into a complete pipeline and test them on one of the sample datasets.\n",
    "\n",
    " 2. **Implement Alternative Clustering:**\n",
    "    Integrate and compare alternative clustering methods (e.g., DBSCAN, hierarchical clustering, or GMM).\n",
    "\n",
    " 3. **Final Reporting:**\n",
    "    Generate comprehensive visualizations and compile a final report detailing the methodology, results, and insights.\n",
    "\n",
    " This notebook serves as a living document that will be continuously updated as we progress through the assignment.\n",
    "\n",
    " ---\n",
    "\n",
    " **End of Part 2, Next Part 3**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
