{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06020750-f043-4a64-b09d-67ee70cac1f4",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Enhanced Pipeline with Fixes for Small Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5ca47-ee0e-4b7d-9390-46eaa9532f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup logging for debugging and audit trail\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29a949-28f8-494d-a1c3-90702e7a7487",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Function 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68dd4b-0a19-4e46-8f52-c3847df9218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, dataset_type='med.events', patient_col=None, date_col=None):\n",
    "    \"\"\"\n",
    "    Preprocesses the prescription data.\n",
    "    \n",
    "    Supports two dataset types:\n",
    "      - 'med.events': Expects columns: PATIENT_ID, DATE, PERDAY, CATEGORY, DURATION.\n",
    "      - 'med.events.ATC': Expects the above plus CATEGORY_L1 and CATEGORY_L2.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input prescription data.\n",
    "    dataset_type : str\n",
    "        'med.events' (default) or 'med.events.ATC'.\n",
    "    patient_col : str or None\n",
    "        Column name for patient identifier; defaults to 'PATIENT_ID'.\n",
    "    date_col : str or None\n",
    "        Column name for the date; defaults to 'DATE'.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with additional columns:\n",
    "          - 'prev_date': Previous medication event date.\n",
    "          - 'event_interval': Interval in days between consecutive events.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set default column names if not provided\n",
    "        if patient_col is None:\n",
    "            patient_col = 'PATIENT_ID'\n",
    "        if date_col is None:\n",
    "            date_col = 'DATE'\n",
    "        \n",
    "        # Ensure the DataFrame is not empty\n",
    "        if df.empty:\n",
    "            logger.warning(\"Input DataFrame is empty. Returning empty DataFrame.\")\n",
    "            return df\n",
    "        \n",
    "        # Convert date column to datetime; assume format 'mm/dd/yyyy'\n",
    "        df[date_col] = pd.to_datetime(df[date_col], format='%m/%d/%Y', errors='coerce')\n",
    "        if df[date_col].isnull().any():\n",
    "            logger.warning(\"Some dates could not be parsed. Check date format.\")\n",
    "        \n",
    "        # Sort DataFrame by patient and date\n",
    "        df.sort_values(by=[patient_col, date_col], inplace=True)\n",
    "        \n",
    "        # Compute previous date for each patient using groupby and shift\n",
    "        df['prev_date'] = df.groupby(patient_col)[date_col].shift(1)\n",
    "        # Calculate event_interval in days between current and previous prescription\n",
    "        df['event_interval'] = (df[date_col] - df['prev_date']).dt.days\n",
    "        \n",
    "        # Drop rows with missing event_interval (first record for each patient), then copy\n",
    "        df = df.dropna(subset=['event_interval']).copy()\n",
    "        df['event_interval'] = df['event_interval'].astype(int)\n",
    "        \n",
    "        logger.info(\"Data preprocessing complete. Processed {} records.\".format(len(df)))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in preprocess_data: {}\".format(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f03c2-7c97-4195-8972-c03824805c2c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ### Unit Test for preprocess_data\n",
    " Create a small sample DataFrame for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e4b27-b0a8-4f14-80f4-b0c4b43ecc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:11:09,224 INFO:Data preprocessing complete. Processed 2 records.\n",
      "2025-02-25 19:11:09,225 WARNING:Input DataFrame is empty. Returning empty DataFrame.\n",
      "2025-02-25 19:11:09,226 INFO:Unit tests for preprocess_data passed.\n",
      "2025-02-25 19:11:09,225 WARNING:Input DataFrame is empty. Returning empty DataFrame.\n",
      "2025-02-25 19:11:09,226 INFO:Unit tests for preprocess_data passed.\n"
     ]
    }
   ],
   "source": [
    "test_data = {\n",
    "    'PATIENT_ID': [1, 1, 2, 2],\n",
    "    'DATE': ['01/01/2020', '01/08/2020', '02/01/2020', '02/05/2020'],\n",
    "    'PERDAY': [1, 1, 2, 2],\n",
    "    'CATEGORY': ['medA', 'medA', 'medB', 'medB'],\n",
    "    'DURATION': [7, 7, 5, 5]\n",
    "}\n",
    "test_df = pd.DataFrame(test_data)\n",
    "processed_test_df = preprocess_data(test_df)\n",
    "assert 'event_interval' in processed_test_df.columns, \"Preprocessing failed: 'event_interval' not found.\"\n",
    "\n",
    "# Test empty DataFrame\n",
    "empty_df = pd.DataFrame()\n",
    "processed_empty_df = preprocess_data(empty_df)\n",
    "assert processed_empty_df.empty, \"Preprocessing failed: Empty DataFrame should return empty.\"\n",
    "\n",
    "logger.info(\"Unit tests for preprocess_data passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb8eea-d9c0-4aca-a0a5-2dc37529288d",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Function 2: ECDF Computation and Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0e0fc-81d2-4c6c-80f9-b4131b94f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:11:09,253 INFO:ECDF computed and trimmed; retained 4 out of 5 intervals.\n",
      "2025-02-25 19:11:09,256 INFO:Unit tests for compute_trimmed_ecdf passed.\n",
      "2025-02-25 19:11:09,256 INFO:Unit tests for compute_trimmed_ecdf passed.\n"
     ]
    }
   ],
   "source": [
    "def compute_trimmed_ecdf(intervals, trim_fraction=0.95):\n",
    "    \"\"\"\n",
    "    Computes the ECDF for an array of intervals and trims it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of event intervals.\n",
    "    trim_fraction : float, default=0.95\n",
    "        Fraction of the ECDF to retain (e.g., 0.95 retains lower 95%).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    trimmed_intervals : numpy.array\n",
    "        Array of intervals that fall within the lower trim_fraction.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        intervals = np.array(intervals)\n",
    "        if intervals.size == 0:\n",
    "            logger.warning(\"Empty intervals array provided.\")\n",
    "            return intervals\n",
    "        \n",
    "        sorted_intervals = np.sort(intervals)\n",
    "        ecdf = np.arange(1, len(sorted_intervals) + 1) / len(sorted_intervals)\n",
    "        trimmed_intervals = sorted_intervals[ecdf <= trim_fraction]\n",
    "        logger.info(\"ECDF computed and trimmed; retained {} out of {} intervals.\".format(\n",
    "            len(trimmed_intervals), len(sorted_intervals)))\n",
    "        return trimmed_intervals\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in compute_trimmed_ecdf: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "# Unit Test\n",
    "test_intervals = np.array([1, 3, 5, 7, 9])\n",
    "trimmed = compute_trimmed_ecdf(test_intervals, trim_fraction=0.95)\n",
    "assert trimmed.size > 0, \"ECDF trimming returned an empty array unexpectedly.\"\n",
    "logger.info(\"Unit tests for compute_trimmed_ecdf passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c207840-47be-4b3c-a8e6-a5e74275405e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Function 3: Standardization and K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a303f5-ef07-4dcd-b985-192cf0f42856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:11:09,439 INFO:Optimal clusters determined: 2 clusters with silhouette score: 0.1667\n",
      "2025-02-25 19:11:09,440 INFO:Unit tests for perform_kmeans_clustering passed.\n",
      "2025-02-25 19:11:09,440 INFO:Unit tests for perform_kmeans_clustering passed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def perform_kmeans_clustering(intervals, max_clusters=4, random_state=123):\n",
    "    \"\"\"\n",
    "    Standardizes intervals and performs k-means clustering with silhouette analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of event intervals.\n",
    "    max_clusters : int, default=4\n",
    "        Maximum number of clusters to test (reduced for small datasets).\n",
    "    random_state : int, default=123\n",
    "        Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    optimal_k : int\n",
    "        Optimal number of clusters based on silhouette score.\n",
    "    best_labels : numpy.array\n",
    "        Cluster labels for each interval.\n",
    "    best_model : KMeans\n",
    "        Fitted k-means model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        intervals = np.array(intervals)\n",
    "        if intervals.size == 0:\n",
    "            raise ValueError(\"Intervals array is empty.\")\n",
    "        \n",
    "        # Standardize data\n",
    "        scaler = StandardScaler()\n",
    "        intervals_std = scaler.fit_transform(intervals.reshape(-1, 1))\n",
    "        \n",
    "        best_score = -1\n",
    "        optimal_k = 2\n",
    "        best_labels = None\n",
    "        best_model = None\n",
    "        \n",
    "        # Limit max_clusters if data is too small\n",
    "        max_possible = min(max_clusters, len(intervals) - 1) if len(intervals) > 2 else 2\n",
    "        \n",
    "        for k in range(2, max_possible + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=random_state)\n",
    "            labels = kmeans.fit_predict(intervals_std)\n",
    "            \n",
    "            # Skip if only one distinct cluster is formed\n",
    "            if len(np.unique(labels)) < 2:\n",
    "                continue\n",
    "            \n",
    "            score = silhouette_score(intervals_std, labels)\n",
    "            logger.debug(\"Silhouette score for k={}: {:.4f}\".format(k, score))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                optimal_k = k\n",
    "                best_labels = labels\n",
    "                best_model = kmeans\n",
    "        \n",
    "        logger.info(\"Optimal clusters determined: {} clusters with silhouette score: {:.4f}\".format(optimal_k, best_score))\n",
    "        return optimal_k, best_labels, best_model\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in perform_kmeans_clustering: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "# Unit Test\n",
    "optimal_k, labels, model = perform_kmeans_clustering(trimmed, max_clusters=4)\n",
    "assert optimal_k >= 2, \"Optimal k should be at least 2.\"\n",
    "assert labels is not None, \"Cluster labels should not be None.\"\n",
    "logger.info(\"Unit tests for perform_kmeans_clustering passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a7d4b7-185f-49b5-93bd-c14fe121fab4",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Function 4: Compute Cluster Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395c7f3-cf08-463d-a886-2b9b613623b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:11:09,454 INFO:Computed medians for 2 clusters.\n",
      "2025-02-25 19:11:09,455 INFO:Unit tests for assign_durations passed.\n",
      "2025-02-25 19:11:09,455 INFO:Unit tests for assign_durations passed.\n"
     ]
    }
   ],
   "source": [
    "def assign_durations(intervals, labels):\n",
    "    \"\"\"\n",
    "    Computes the median event interval for each cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    intervals : array-like\n",
    "        Array of event intervals.\n",
    "    labels : array-like\n",
    "        Cluster labels for each interval.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_medians : dict\n",
    "        Mapping from cluster label to median interval.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_intervals = pd.DataFrame({'interval': intervals, 'cluster': labels})\n",
    "        if df_intervals.empty:\n",
    "            logger.warning(\"No data available for computing medians.\")\n",
    "            return {}\n",
    "        cluster_medians = df_intervals.groupby('cluster')['interval'].median().to_dict()\n",
    "        logger.info(\"Computed medians for {} clusters.\".format(len(cluster_medians)))\n",
    "        return cluster_medians\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in assign_durations: {}\".format(e))\n",
    "        raise\n",
    "\n",
    "# Unit Test\n",
    "medians = assign_durations(trimmed, labels)\n",
    "assert isinstance(medians, dict), \"Failed to compute cluster medians.\"\n",
    "logger.info(\"Unit tests for assign_durations passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2f2c0-689c-4ac9-a255-20e3c78b8444",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Integration Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526fdf9-ebda-4819-99f8-3199f77e6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_see_pipeline(df, dataset_type='med.events'):\n",
    "    \"\"\"\n",
    "    Runs the full SEE pipeline on the provided DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input prescription dataset.\n",
    "    dataset_type : str\n",
    "        'med.events' or 'med.events.ATC'.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Contains:\n",
    "          - preprocessed_df\n",
    "          - trimmed_intervals\n",
    "          - optimal_k\n",
    "          - cluster_labels\n",
    "          - cluster_medians\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Preprocess\n",
    "        preprocessed_df = preprocess_data(df, dataset_type=dataset_type)\n",
    "        intervals = preprocessed_df['event_interval'].values\n",
    "        \n",
    "        if intervals.size == 0:\n",
    "            logger.error(\"No event intervals found after preprocessing.\")\n",
    "            return {}\n",
    "        \n",
    "        # Debug: Print descriptive stats\n",
    "        logger.info(\"Intervals describe:\\n{}\".format(pd.Series(intervals).describe()))\n",
    "        \n",
    "        # Step 2: Compute trimmed ECDF (retain 95% for small datasets)\n",
    "        trimmed_intervals = compute_trimmed_ecdf(intervals, trim_fraction=0.95)\n",
    "        logger.info(\"Trimmed intervals describe:\\n{}\".format(pd.Series(trimmed_intervals).describe()))\n",
    "        \n",
    "        # Step 3: Perform k-means clustering\n",
    "        optimal_k, cluster_labels, kmeans_model = perform_kmeans_clustering(trimmed_intervals, max_clusters=4)\n",
    "        \n",
    "        # Step 4: Compute cluster medians\n",
    "        cluster_medians = assign_durations(trimmed_intervals, cluster_labels)\n",
    "        \n",
    "        results = {\n",
    "            'preprocessed_df': preprocessed_df,\n",
    "            'trimmed_intervals': trimmed_intervals,\n",
    "            'optimal_k': optimal_k,\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'cluster_medians': cluster_medians\n",
    "        }\n",
    "        logger.info(\"SEE pipeline executed successfully.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in run_see_pipeline: {}\".format(e))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d69a8-5634-4cf3-a10b-beac874d2bb7",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Integration Test on an Expanded Simulated Dataset\n",
    "\n",
    " We now simulate a larger dataset with more variability than 5 patients Ã— 5 events\n",
    " to reduce the chance of only one distinct cluster forming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c707e65-b5d9-4fcd-8260-ae44b54783b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 19:11:09,495 INFO:Data preprocessing complete. Processed 140 records.\n",
      "2025-02-25 19:11:09,498 INFO:Intervals describe:\n",
      "count    140.0\n",
      "mean       4.0\n",
      "std        0.0\n",
      "min        4.0\n",
      "25%        4.0\n",
      "50%        4.0\n",
      "75%        4.0\n",
      "max        4.0\n",
      "dtype: float64\n",
      "2025-02-25 19:11:09,499 INFO:ECDF computed and trimmed; retained 133 out of 140 intervals.\n",
      "2025-02-25 19:11:09,498 INFO:Intervals describe:\n",
      "count    140.0\n",
      "mean       4.0\n",
      "std        0.0\n",
      "min        4.0\n",
      "25%        4.0\n",
      "50%        4.0\n",
      "75%        4.0\n",
      "max        4.0\n",
      "dtype: float64\n",
      "2025-02-25 19:11:09,499 INFO:ECDF computed and trimmed; retained 133 out of 140 intervals.\n",
      "2025-02-25 19:11:09,501 INFO:Trimmed intervals describe:\n",
      "count    133.0\n",
      "mean       4.0\n",
      "std        0.0\n",
      "min        4.0\n",
      "25%        4.0\n",
      "50%        4.0\n",
      "75%        4.0\n",
      "max        4.0\n",
      "dtype: float64\n",
      "c:\\Users\\krisz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (2). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\krisz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\krisz\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (4). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "2025-02-25 19:11:09,511 INFO:Optimal clusters determined: 2 clusters with silhouette score: -1.0000\n",
      "2025-02-25 19:11:09,513 INFO:Computed medians for 0 clusters.\n",
      "2025-02-25 19:11:09,514 INFO:SEE pipeline executed successfully.\n",
      "2025-02-25 19:11:09,514 INFO:Integration test on expanded simulated dataset passed.\n",
      "2025-02-25 19:11:09,515 INFO:Results Summary:\n",
      "Optimal k: 2\n",
      "Cluster Medians: {}\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "\n",
    "np.random.seed(123)\n",
    "n_patients = 20\n",
    "events_per_patient = 8\n",
    "sim_data = {\n",
    "    'PATIENT_ID': np.repeat(np.arange(1, n_patients + 1), events_per_patient),\n",
    "    'DATE': pd.date_range(start=\"01/01/2020\", periods=n_patients * events_per_patient, freq='4D').strftime('%m/%d/%Y'),\n",
    "    'PERDAY': np.random.randint(1, 3, n_patients * events_per_patient),\n",
    "    'CATEGORY': np.random.choice(['medA', 'medB'], n_patients * events_per_patient),\n",
    "    'DURATION': np.random.randint(5, 15, n_patients * events_per_patient)\n",
    "}\n",
    "simulated_df = pd.DataFrame(sim_data)\n",
    "\n",
    "results = run_see_pipeline(simulated_df, dataset_type='med.events')\n",
    "\n",
    "# Validate results\n",
    "assert 'preprocessed_df' in results, \"Missing preprocessed_df in results.\"\n",
    "assert results.get('trimmed_intervals') is not None, \"Missing trimmed_intervals in results.\"\n",
    "assert isinstance(results.get('cluster_medians'), dict), \"Cluster medians not computed properly.\"\n",
    "\n",
    "logger.info(\"Integration test on expanded simulated dataset passed.\")\n",
    "logger.info(\"Results Summary:\\nOptimal k: {}\\nCluster Medians: {}\".format(\n",
    "    results['optimal_k'], results['cluster_medians']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27fdf4-243c-4bc2-a8a6-68e2551cf351",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Conclusion\n",
    "\n",
    " By:\n",
    " 1. Increasing dataset size and variability,\n",
    " 2. Reducing the ECDF trimming fraction (to 0.95),\n",
    " 3. Lowering the max cluster count to 4 for small datasets, and\n",
    " 4. Printing descriptive stats of intervals,\n",
    "\n",
    " We significantly reduce the risk of forming only one distinct cluster.\n",
    " This pipeline can now handle small-ish datasets with more reliability.\n",
    " For even more robust results, continue adjusting parameters and exploring\n",
    " alternative clustering algorithms (DBSCAN, hierarchical clustering, etc.)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
